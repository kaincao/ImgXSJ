---
title: 2025-11-10Google 解决了AI 灾难性遗忘的问题
tags: 新建,模板,小书匠
category: /小书匠/日记/2025-11
grammar_cjkRuby: true
---


Google 解决了AI 灾难性遗忘的问题 ?

人脑通过 Neuroplasticity（神经可塑性） 实现“多层次更新”：

一些神经连接变化得快（短期学习）
一些变化得慢（长期记忆）
它们在不同频率上更新，却协同运作

Google 的目标是：

让AI模型也具备“多时间尺度、多层嵌套的学习能力”，像人脑那样持续成长。

让AI拥有和人类一样的长期记忆能力...

AI 模型在学习新任务时，会忘记旧知识。

这个现象被称为:灾难性遗忘（Catastrophic Forgetting）。

📘 举个例子：

你让一个语言模型先学英语写作；
再让它去学编程；
结果它编程变强了，但英文写作能力下降。

传统解决方法（例如冻结部分参数或正则化）只是“权宜之计”，并没有真正让模型学会像人类那样“持续学习”。

Google Research 团队提出的 Nested Learning（嵌套学习），旨在解决这一核心问题，让AI具备像人类一样的“持续学习”能力。

核心理念：模型不是单一学习体，而是多层学习系统

传统神经网络把“模型架构”和“优化器”分开对待，而Nested Learning的关键思想是：

模型内部其实由多个相互嵌套的学习过程组成，每一层都有自己独立的学习节奏与目标。

这种架构让模型具备多层学习机制：

高频层 → 负责快速学习短期任务
中频层 → 整合知识、适应场景变化
低频层 → 稳定长期知识，防止遗忘

这就像人类记忆系统中，短期记忆、工作记忆与长期记忆的协作机制。

它的意思是：AI 模型不是一个整体学习系统，而是一组套在一起的小学习系统。

想象一个学习者：

有一层负责快速记住新知识（短期记忆）
有一层负责整合最近学的内容（中期记忆）
还有一层保存重要的、稳定的知识（长期记忆）

这三层是嵌套的，就像洋葱一样。每层的“学习速度”不同——外层学得快、内层学得慢。

这样的结构让 AI 像人脑一样，在多层次上学习与更新。